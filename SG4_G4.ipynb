{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Load & basic preparation**\n",
        "The dataset is loaded, inspected, and prepared by combining titles and bodies into a single text field for analysis."
      ],
      "metadata": {
        "id": "hD903MSaW3i-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpQ2Fl0RlUMh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# Load data\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(\"Dutch_Migration_News.csv\")\n",
        "\n",
        "# Quick exploration\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "display(df.head(3))\n",
        "\n",
        "# Basic cleaning / preparation\n",
        "# Drop index column if present\n",
        "if \"Unnamed: 0\" in df.columns:\n",
        "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "# Check required text columns\n",
        "expected_cols = {\"title\", \"body\"}\n",
        "missing = expected_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Dataset is missing columns: {missing}\")\n",
        "\n",
        "# Fill missing text values\n",
        "df[\"title\"] = df[\"title\"].fillna(\"\")\n",
        "df[\"body\"] = df[\"body\"].fillna(\"\")\n",
        "\n",
        "# Combine title and body\n",
        "df[\"text\"] = (df[\"title\"].str.strip() + \". \" + df[\"body\"].str.strip()).str.strip()\n",
        "\n",
        "# Remove very short texts\n",
        "df[\"text_len\"] = df[\"text\"].str.len()\n",
        "df = df[df[\"text_len\"] >= 50].copy()\n",
        "\n",
        "print(\"\\nAfter basic cleaning:\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Text length stats:\")\n",
        "print(df[\"text_len\"].describe()[[\"min\", \"25%\", \"50%\", \"mean\", \"75%\", \"max\"]])\n",
        "\n",
        "# Keep relevant columns\n",
        "keep_cols = [\"outlet\", \"year\", \"authors\", \"title\", \"text\"]\n",
        "df = df[keep_cols].copy()\n",
        "\n",
        "display(df.sample(3, random_state=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text preprocessing**\n",
        "Text preprocessing is applied to normalize the texts and prepare them for topic modeling and sentiment analysis."
      ],
      "metadata": {
        "id": "s2wrAhl-VOUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "dutch_stopwords = set(stopwords.words(\"dutch\"))\n",
        "english_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Combine Dutch and English stopwords\n",
        "all_stopwords = dutch_stopwords.union(english_stopwords)\n",
        "\n",
        "print(\"Number of Dutch stopwords:\", len(dutch_stopwords))\n",
        "print(\"Number of English stopwords:\", len(english_stopwords))\n",
        "print(\"Total stopwords (combined):\", len(all_stopwords))\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Basic Dutch text preprocessing:\n",
        "    - lowercasing\n",
        "    - remove punctuation and numbers\n",
        "    - tokenization by whitespace\n",
        "    - remove Dutch + English stopwords\n",
        "    - keep tokens with length >= 3\n",
        "    \"\"\"\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r\"[^a-zà-ÿ\\s]\", \" \", text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Stopword removal and length filtering\n",
        "    tokens = [\n",
        "        token for token in tokens\n",
        "        if token not in all_stopwords and len(token) >= 3\n",
        "    ]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing\n",
        "df[\"tokens\"] = df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "# Create cleaned text strings\n",
        "df[\"clean_text\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "# Sanity checks\n",
        "print(\"Example tokens:\")\n",
        "print(df[\"tokens\"].iloc[0][:30])\n",
        "\n",
        "print(\"\\nExample clean_text:\")\n",
        "print(df[\"clean_text\"].iloc[0][:300])\n",
        "\n",
        "empty_docs = (df[\"clean_text\"].str.len() == 0).sum()\n",
        "print(\"\\nEmpty documents after preprocessing:\", empty_docs)"
      ],
      "metadata": {
        "id": "eMZIVvugcaIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modeling (TF-IDF + LDA)**\n",
        "Topic modeling is performed using TF-IDF features and Latent Dirichlet Allocation (LDA) to identify dominant themes in migration-related news coverage."
      ],
      "metadata": {
        "id": "xmxrpoohY3mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_df=0.9,       # high-frequency term filtering\n",
        "    min_df=10,        # low-frequency term filtering\n",
        "    max_features=5000 # feature space size\n",
        ")\n",
        "\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
        "\n",
        "# LDA model\n",
        "N_TOPICS = 8  # number of topics\n",
        "\n",
        "lda_model = LatentDirichletAllocation(\n",
        "    n_components=N_TOPICS,\n",
        "    random_state=42,\n",
        "    learning_method=\"batch\"\n",
        ")\n",
        "\n",
        "lda_model.fit(X_tfidf)\n",
        "\n",
        "# Topic inspection\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "def display_topics(model, feature_names, n_top_words=12):\n",
        "    topics = {}\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features = [\n",
        "            feature_names[i]\n",
        "            for i in topic.argsort()[:-n_top_words - 1:-1]\n",
        "        ]\n",
        "        topics[f\"Topic {topic_idx}\"] = top_features\n",
        "    return topics\n",
        "\n",
        "topics = display_topics(lda_model, feature_names)\n",
        "\n",
        "for topic, words in topics.items():\n",
        "    print(f\"\\n{topic}:\")\n",
        "    print(\", \".join(words))\n",
        "\n",
        "# Dominant topic per article\n",
        "doc_topic_dist = lda_model.transform(X_tfidf)\n",
        "df[\"dominant_topic\"] = doc_topic_dist.argmax(axis=1)\n",
        "\n",
        "df[\"dominant_topic\"].value_counts().sort_index()"
      ],
      "metadata": {
        "id": "xWNvrpQNY4zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentiment Analysis**\n",
        "Sentiment scores are computed on the original article texts using a lexicon-based approach and aggregated per dominant topic."
      ],
      "metadata": {
        "id": "g3hBjDn4iMZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# VADER setup\n",
        "nltk.download(\"vader_lexicon\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Document-level sentiment\n",
        "def get_sentiment(text):\n",
        "    \"\"\"\n",
        "    Computes compound sentiment score using VADER.\n",
        "    Range: [-1, 1]\n",
        "    \"\"\"\n",
        "    return sia.polarity_scores(text)[\"compound\"]\n",
        "\n",
        "df[\"sentiment\"] = df[\"text\"].apply(get_sentiment)\n",
        "\n",
        "# Distribution check\n",
        "df[\"sentiment\"].describe()"
      ],
      "metadata": {
        "id": "CRHo9vgcIWyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment per topic\n",
        "topic_sentiment = (\n",
        "    df.groupby(\"dominant_topic\")[\"sentiment\"]\n",
        "      .agg([\"mean\", \"median\", \"count\"])\n",
        "      .sort_index()\n",
        ")\n",
        "\n",
        "topic_sentiment"
      ],
      "metadata": {
        "id": "LUIYUCnQI7JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment categories\n",
        "def sentiment_label(score):\n",
        "    if score > 0.05:\n",
        "        return \"positive\"\n",
        "    elif score < -0.05:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "df[\"sentiment_label\"] = df[\"sentiment\"].apply(sentiment_label)\n",
        "\n",
        "sentiment_distribution = (\n",
        "    df.groupby([\"dominant_topic\", \"sentiment_label\"])\n",
        "      .size()\n",
        "      .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "sentiment_distribution"
      ],
      "metadata": {
        "id": "a1ZdejX5JBKI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}